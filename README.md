<h1 align="center">ğŸ’³ Monitoramento de TransaÃ§Ãµes Financeiras com PySpark + Airflow</h1>

<p align="center">
  <strong>Projeto de Engenharia de Dados</strong> com ingestÃ£o, processamento e enriquecimento de transaÃ§Ãµes financeiras simuladas.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/status-concluÃ­do-brightgreen" />
  <img src="https://img.shields.io/badge/python-3.10-blue" />
  <img src="https://img.shields.io/badge/spark-4.0-orange" />
  <img src="https://img.shields.io/badge/airflow-2.7+-green" />
  <img src="https://img.shields.io/badge/terraform-Azure-blueviolet" />
</p>

---

## ğŸ§­ VisÃ£o Geral

Este pipeline ETL simula transaÃ§Ãµes bancÃ¡rias e as **enriquece com dados pÃºblicos** para identificar padrÃµes e facilitar anÃ¡lises e realizar um modelo de machine learning que nos ajuda a prever fraudes.

Principais recursos:

- ğŸ”„ OrquestraÃ§Ã£o com **Apache Airflow**  
- âš¡ Processamento distribuÃ­do com **PySpark**  
- ğŸ”’ SeguranÃ§a com mascaramento de dados sensÃ­veis  
- ğŸ“ˆ Escalabilidade e observabilidade  
- ğŸ§± Arquitetura em camadas (**Bronze, Silver, Gold**)

---

## âš™ï¸ Tecnologias Utilizadas

- ğŸ **Python 3.10.12**
- ğŸ”¥ **Apache Spark 4.0**
- ğŸŒ¬ï¸ **Apache Airflow 2.7+**
- ğŸ˜ **PySpark**
- ğŸ³ **Docker + Docker Compose**
- ğŸ“¦ **Kaggle Datasets + IBGE (dados pÃºblicos)**
- ğŸ“Š **Streamlit**
- ğŸ˜ **PostgreSQL**
- â˜ï¸ **Azure Virtual Machine**
- ğŸ“œ **Terraform** (Infra como CÃ³digo)

---

## ğŸ§± Arquitetura em Camadas (Medallion Architecture)

| Camada  | DescriÃ§Ã£o |
|---------|----------|
| ğŸŸ¤ **Bronze** | Dados brutos simulados com `Faker` |
| âšª **Silver** | Dados limpos, validados e enriquecidos |
| ğŸŸ¡ **Gold**   | Dados mascarados e integrados com dados pÃºblicos |

---

## ğŸ“ Estrutura do Projeto

```bash
.
â”œâ”€â”€ dags/                       # DAGs do Airflow
â”œâ”€â”€ data/                       # Dados particionados (Bronze, Silver, Gold)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_silver.py           # Testes de funÃ§Ãµes da camada Silver
â”‚   â””â”€â”€ test_gold.py             # Testes de funÃ§Ãµes da camada Gold
â”‚
â”œâ”€â”€ scripts/                    # Scripts PySpark
â”œâ”€â”€ docker-compose.yml          # OrquestraÃ§Ã£o de containers
â”œâ”€â”€ Dockerfile.airflow.spark    # Airflow + Spark
â”œâ”€â”€ Dockerfile.streamlit        # Dashboard Streamlit
â”œâ”€â”€ requirements.txt            # DependÃªncias
â”œâ”€â”€ secrets/                    # ContÃ©m kaggle.json (nÃ£o versionado)
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ terraform/              # CÃ³digo IaC para Azure
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â”œâ”€â”€ terraform.tfvars
â”‚       â””â”€â”€ outputs.tf
â””â”€â”€ README.md



ğŸ“Š Dados Utilizados

SimulaÃ§Ã£o: Geradas com Faker

Kaggle:

creditcard.csv

bank_marketing_full.csv

IBGE: MunicÃ­pios e Estados

Receita Federal (simulada): Nomes de bancos

Lembrando que podemos executar de forma local ou cloud (Azure)


â˜ï¸ Deploy na Azure com Terraform
1ï¸âƒ£ Instalar DependÃªncias

Azure 

Instale a Azure CLI (caso ainda nÃ£o tenha):
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

FaÃ§a login na Azure
az login --use-device-code
az login
az account show


Se vocÃª tiver mÃºltiplas assinaturas:
az account set --subscription "ID-ou-Nome-da-Sua-Subscription"


# Terraform
Passo a passo para instalar o Terraform (Linux/WSL)
Instalar dependÃªncias:

sudo apt-get update && sudo apt-get install -y gnupg software-properties-common curl unzip
Adicionar o repositÃ³rio oficial do Terraform:

curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/hashicorp.gpg
sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
Instalar o Terraform:

sudo apt-get update && sudo apt-get install terraform
Verificar instalaÃ§Ã£o:

terraform -v

2ï¸âƒ£ Autenticar na Azure
az login --use-device-code
az account set --subscription "SUA_SUBSCRIPTION"

3ï¸âƒ£ Pegar arquivos do Git

git clone https://github.com/Leoosantoszl/ETL-Transacoes-Financeiras.git
cd projeto

4ï¸âƒ£ Configurar kaggle.json
Fora da VM:
VÃ¡ atÃ©: https://www.kaggle.com/settings
faÃ§a login com sua conta

Na seÃ§Ã£o API, clique em Create New API Token

Isso irÃ¡ baixar kaggle.json

Agora podemos seguir com o projeto:

5ï¸âƒ£ Provisionar Infraestrutura

cd projeto/IAC/terraform
terraform init
terraform plan
terraform apply

Isso criarÃ¡ a VM com Docker, Docker Compose e o projeto jÃ¡ configurado via cloud-init.

6ï¸âƒ£ - Conectar na VM, instalar dependencias, e rodar projeto
ssh azureuser@IP_DA_VM

Instale as dependencias

cd projeto

pip install -r requirements.txt

ApÃ³s instalar as dependencias na sua VM, vÃ¡ na sua maquina onde foi baixado o arquivo kaggle.json
Rode o comando para transferir o arquivo para sua VM, lembrando de alterar o caminho do arquivo e o IP da VM
normalmente por padrÃ£o a chave rsa fica nesse diretorio(sÃ³ confira se a sua estÃ¡)

scp -i ~/.ssh/id_rsa (caminho arquivo)/kaggle.json azureuser@(IP VM):/home/azureuser/

E coloque sua senha rsa
Agora dentro da VM:
Mova o arquivo para o diretorio secrets dentro do projeto e de permissÃ£o para ele:

mv ~/kaggle.json projeto/secrets/
chmod 600 ~/projeto/secrets/kaggle.json

Depois acesse a pasta do projeto e rode o comando

sudo docker-compose up -d

A aplicaÃ§Ã£o jÃ¡ estarÃ¡ rodando e o Airflow acessÃ­vel pelo IP pÃºblico na porta 8080

apos o processamento da camada Gold, entre no streamlit para ver os resultados.
acesse: IP_VM:8051
os graficos com os insights do projeto estaram disponivel la.

Agora de forma local:

ğŸš€ Como Executar Localmente
1ï¸âƒ£ Clonar o RepositÃ³rio
git clone https://github.com/Leoosantoszl/ETL-Transacoes-Financeiras.git
cd projeto

2ï¸âƒ£ Criar e Ativar Ambiente Virtual

python3 -m venv airflow-env
source airflow-env/bin/activate
pip install -r requirements.txt

3ï¸âƒ£ Configurar kaggle.json

VÃ¡ atÃ©: https://www.kaggle.com/settings

Na seÃ§Ã£o API, clique em Create New API Token

Isso irÃ¡ baixar kaggle.json

Mova o arquivo para:

mkdir -p secrets
mv ~/Downloads/kaggle.json secrets/
chmod 600 secrets/kaggle.json

4ï¸âƒ£ Subir Containers
docker compose build (Caso nÃ£o tenha as imagens que veem no projeto)
docker compose up -d


Acesse Airflow: http://localhost:8080
UsuÃ¡rio: admin
Senha: admin

Ative e execute a DAG pipeline_transacoes_pyspark

ğŸ§ª Testes

O projeto possui testes automatizados para as camadas Silver e Gold, garantindo que a transformaÃ§Ã£o e o enriquecimento dos dados estejam corretos.

1. PrÃ©-requisitos

Ter o Python 3.10+ instalado.

Ter o PySpark instalado (pip install pyspark).

Ter o pytest instalado (pip install pytest).

2. Estrutura dos testes

scripts/tests/test_silver.py: Testa funÃ§Ãµes de limpeza e enriquecimento da camada Silver, incluindo filtros de CPF, valores e nomes de bancos.

scripts/tests/test_gold.py: Testa funÃ§Ãµes da camada Gold, como mascaramento de CPF e reorganizaÃ§Ã£o de colunas.

rode o comando
PYTHONPATH=$(pwd) pytest



ğŸ‘¨â€ğŸ’» Autor
Leonardo Oliveira dos Santos
Engenheiro de Dados | Python | PySpark | Airflow | Docker | Terraform |
LinkedIn https://www.linkedin.com/in/leonardo-oliveira-20083b1a2/  â€¢ GitHub https://github.com/Leoosantoszl?tab=repositories


